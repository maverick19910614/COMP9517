{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jian Gao<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Document https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, pytorch version is \"1.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not install Pytorch before, please follow below link:<br>\n",
    "https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input of Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, we use tensor as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1, 2, 3]\n",
    "X = torch.tensor(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions are samiliar with numpy, like ones, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3, 3) # return 3x3 (matrix) tensor, all elements are one, float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 3) # return 3x3 (matrix) tensor, all elements are zero, float type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN layer in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer (2 dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'), you can want to see more detail, see https://pytorch.org/docs/stable/nn.html?highlight=conv#torch.nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we have a 32x32x3 image, we want use 5x5x3 filter with stride = 1 to convolve, we will get 28x28x1 avtivation map, the depth of filter will same as the depth of image, if we want 6 different activation maps, we need 6 different 5x5x3 filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/conv1.png\" width = \"450\" align = 'left'><img src = \"img/conv2.png\" width = \"450\" align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images from cs231n slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a virtural image with 32x32x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.randn(32, 32, 3) # random initial\n",
    "image.size() # print the size of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we need to define a 2-d convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstconv = torch.nn.Conv2d(3, 6, 5) # input channel is 3, output channel is 6, and using 5x5 filter or kernel\n",
    "# In 2 dimension convolution, channel represents depth, e.g. 5x5x3 image, the channel size is 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch, the input of convolution layer is defined as (N, $C_{in}$, $H_{in}$, $W_{in}$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N represents batch_size, now we have one image, so N = 1 <br>\n",
    "C represents the channel of image <br>\n",
    "H represents the height of image <br>\n",
    "W represents the width of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use function \"view()\" to reshape the tensor size, same as \"np.reshape()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = image.view(1, 3, 32, 32)\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we can feed our image to this convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfirstconv(image).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the output activation maps with size 1x6x28x28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer (2 dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False), you can want to see more detail, see https://pytorch.org/docs/stable/nn.html?highlight=maxpool2d#torch.nn.MaxPool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/pooling.png\" width = \"450\" align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from COMP9517 slides "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a small image with 4x4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 2., 4.],\n",
       "          [5., 6., 7., 8.],\n",
       "          [3., 2., 1., 0.],\n",
       "          [1., 2., 3., 4.]]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.tensor([[1., 1., 2., 4.], [5., 6., 7., 8.], [3., 2., 1., 0.], [1., 2., 3., 4.]])\n",
    "image = image.view(1, 1, 4, 4)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstmaxpooling = torch.nn.MaxPool2d(2, stride = 2) # 2 means 2x2 filters or kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[6., 8.],\n",
       "          [3., 4.]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfirstmaxpooling(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want 2 dimension dropout layer, we can use torch.nn.Dropout2d(p=0.5, inplace=False), you can want to see more detail, see https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout2d <br>\n",
    "If we want 1 dimension dropout layer, we can use torch.nn.Dropout(p=0.5, inplace=False), you can want to see more detail, see https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 1 dimension dropout layer with 50% dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstdropout = torch.nn.Dropout(p = 0.5) # p = 0.5 means 50% probability of an element to be zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current x tensor([1., 2., 3., 4., 5.])\n",
      "after dropout tensor([ 2.,  0.,  6.,  0., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3., 4., 5.])\n",
    "print(\"current x\", x)\n",
    "x = myfirstdropout(x)\n",
    "print(\"after dropout\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some elemennts become zero. The reason the rest of element doubled is using dropout with p=0.5, we will lose half of these activations. And we deactivate dropout during testing.(https://discuss.pytorch.org/t/unclear-behaviour-of-dropout/22890/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/relu.png\" width = \"250\" align = 'left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfirstRelu = torch.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor(100)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(myfirstRelu(torch.tensor(5)))\n",
    "print(myfirstRelu(torch.tensor(100)))\n",
    "print(myfirstRelu(torch.tensor(-50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connection Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use torch.nn.Linear(in_features, out_features, bias=True), you can want to see more detail, see https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in_features means the number of input neuros<br>\n",
    "out_features means the number of output neuros<br>\n",
    "In below image, the in_features is 5, the out_features is 3<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/linear.png\" width = \"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([5])\n",
      "output size: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(\"input size:\", x.size())\n",
    "myfirstlinear = torch.nn.Linear(5, 3)\n",
    "print(\"output size:\", myfirstlinear(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we can create a very samll CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetWork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNetWork, self).__init__()\n",
    "        \n",
    "        # conv layers: feature extractor\n",
    "        # using nn.Sequential can concate layer together and more less code\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 10, kernel_size = 5),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(10, 20, kernel_size = 5),\n",
    "            torch.nn.Dropout2d(0.5),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # fc layers: classifier\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(320, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(50, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # flatten the final output of conv_layers\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc_layers(x)\n",
    "        # softmax can output log probability of each potential classes\n",
    "        return torch.nn.functional.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"src/mnist_train.csv\")\n",
    "train_data = data[:2000]\n",
    "test_data = data[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train data 2000\n",
      "# test data 500\n"
     ]
    }
   ],
   "source": [
    "print(\"# train data\", len(train_data))\n",
    "print(\"# test data\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data['label']\n",
    "train_label = torch.tensor(train_label)\n",
    "test_label = test_data['label']\n",
    "test_label = torch.tensor(test_label.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, pixel value is in range [0, 255], we can do simple normalizatoin into [0, 1], it may let calculation faster and easier to converage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop([\"label\"], axis = 1)\n",
    "train_data = train_data.to_numpy() / 255\n",
    "test_data = test_data.drop([\"label\"], axis = 1)\n",
    "test_data = test_data.to_numpy() / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Data Into torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data, dtype = torch.float32).view(-1, 1, 28, 28)\n",
    "test_data = torch.tensor(test_data, dtype = torch.float32).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run below code, it may take few minutes to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50 loss: 0.8088286519050598\n",
      "epoch: 100 loss: 0.45997485518455505\n",
      "epoch: 150 loss: 0.3428522050380707\n",
      "epoch: 200 loss: 0.282133013010025\n",
      "epoch: 250 loss: 0.24708323180675507\n",
      "epoch: 300 loss: 0.18074654042720795\n"
     ]
    }
   ],
   "source": [
    "# define our model\n",
    "model = SmallNetWork()\n",
    "# define learning rate\n",
    "learning_rate = 1e-3\n",
    "# define optimizer, you can change to SGD or other optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# define loss function, as this is classification problem, we can use CrossEntropy Loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(300):\n",
    "    # train mode\n",
    "    model.train()\n",
    "    y_pred = model(train_data)\n",
    "    # calculate loss\n",
    "    loss = criterion(y_pred, train_label)\n",
    "    # calculate backpropogation\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    # reset our optimizer\n",
    "    optimizer.zero_grad()\n",
    "    # print loss\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(\"epoch:\", epoch + 1, \"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # test mode, will not use dropout\n",
    "test_pred = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_pred = torch.max(test_pred, dim = 1) # get highest probability classes as final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def visualization(image, label, predict):\n",
    "    pixels = np.array(image, dtype = 'uint8')\n",
    "    image = image.reshape((28, 28))\n",
    "    plt.title(f'label is {label}, predict label is {predict}')\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization(test_data[10], test_label[10], test_pred[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 93.8\n",
      "test acc: 91.0\n"
     ]
    }
   ],
   "source": [
    "model.eval() # will not use dropout\n",
    "train_pred = model(train_data)\n",
    "_, train_pred = torch.max(train_pred, dim = 1)\n",
    "print(\"train acc:\", round((train_label == train_pred).type(torch.float32).mean().item() * 100, 2))\n",
    "print(\"test acc:\", round((test_label == test_pred).type(torch.float32).mean().item() * 100, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
